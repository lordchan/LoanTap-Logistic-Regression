# -*- coding: utf-8 -*-
"""LoanTap_Classification ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1leSVcEFdeP4_QhIgrjzUAMbqVV2vIeDl
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('logistic_regression.csv')

df

df.iloc[:,9:18]

df.iloc[:,18:]

df.shape

df.info()

df.describe()

#For categoric variables, let us see their value counts
df['term'].value_counts()

"""## Data Cleaning - Converting categoric variable to numeric variable"""

df['term'].replace({" 36 months":36, " 60 months":60}, inplace = True)

df['term']

df['grade'].value_counts()

#We are going to convert subgrade from Ordinal values to Numeric values
grade = df['sub_grade'].value_counts().index.sort_values()
grade

len(grade)

# We will assign more points to better grades and it decreases continuously to G5
pd.DataFrame({"grade_name": list(grade), "grade_value": list(range(len(grade),0,-1))})

df["sub_grade"].replace(list(grade), list(range(len(grade),0,-1)), inplace = True)

# Since we already have the subgrade data, we will no more require the Grade data
df.drop(columns = ["grade"], inplace= True)

df['emp_title'].value_counts()

# THis is the output variable, so we will convert it into 0 and 1.
df['loan_status'].replace(['Fully Paid', 'Charged Off'], [1,0], inplace = True)

emp_title_target_enc = df.groupby('emp_title')['loan_status'].agg(['mean', 'count']).reset_index()
emp_title_target_enc

"""## Feature Engineering on employee title. Converting categoric to numeric value using Target Encoding."""

# Target Encoding has a tendency to overfit so we will be smoothening
# Formula : (n*mean_of_category + alpha*global_mean)/(n + alpha)
# ---- where n is the count of rows in the category and alpha is a hyperparameter

global_mean = df['loan_status'].mean()
alpha = 10
emp_title_target_enc['target_enc'] = (emp_title_target_enc['count']*emp_title_target_enc['mean'] + alpha*global_mean)/(emp_title_target_enc['count'] + alpha)

emp_title_target_enc.sort_values( by = 'target_enc')

df = df.merge(emp_title_target_enc, left_on = 'emp_title', right_on = 'emp_title', how = 'left')
df

df.columns

df.drop(columns = ['emp_title', 'mean', 'count'], inplace= True)

df.rename(columns = {'target_enc': 'emp_title'}, inplace = True)

df['emp_length']

df['emp_length'] = df['emp_length'].str.extract('(\d+)')

df

df['home_ownership'].value_counts()

"""## One Hot Encoding on 'Home Ownership' column"""

df['mortage_one_hot'] = 0
df.loc[df['home_ownership'] == 'MORTGAGE','mortage_one_hot'] = 1

df['rent_one_hot'] = 0
df.loc[df['home_ownership'] == 'RENT','rent_one_hot'] = 1

df['own_home'] = 0
df.loc[df['home_ownership'] == 'OWN','own_home'] = 1
df.drop(columns = ['home_ownership', 'mort_acc'], inplace = True)

df

df['verification_status'].value_counts()

df['verification_status'].replace(['Verified', 'Source Verified', 'Not Verified'], [1,1,0], inplace = True)

df['verification_status'].value_counts()

# Issue date is ordinal values, so we will convert it to numeric values
df['issue_d'].value_counts()

df['year'] = df['issue_d'].str.extract('(\d+)').astype('float64')
df['month'] = df['issue_d'].str.slice(0,3).replace(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\
                                     , [1,2,3,4,5,6,7,8,9,10,11,12])/12
df['date']= df['year'] + df['month']

df.drop(columns = ['issue_d', 'year', 'month'], inplace = True)

df.iloc[:,9:18]

df.drop(columns = ['purpose'], inplace = True)

df.drop(columns = ['title'], inplace = True)

df['year'] = df['earliest_cr_line'].str.extract('(\d+)').astype('float64')
df['month'] = df['earliest_cr_line'].str.slice(0,3).replace(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\
                                     , [1,2,3,4,5,6,7,8,9,10,11,12])/12
df['earliest_cr_line_date']= df['year'] + df['month']

df.drop(columns = ['earliest_cr_line', 'year', 'month'], inplace = True)

df.info()

# We will convert pub rec and pub rec bankruptcy into just 0 and 1
df.loc[df['pub_rec'] >= 1, 'pub_rec'] = 1
df.loc[df['pub_rec'] != 1, 'pub_rec'] = 0

df.loc[df['pub_rec_bankruptcies'] >= 1, 'pub_rec_bankruptcies'] = 1
df.loc[df['pub_rec_bankruptcies'] != 1, 'pub_rec_bankruptcies'] = 0

df.info()

# One hot encoding
df['initial_list_status'].replace(['w', 'f'], [0,1], inplace = True)

df['application_type'].value_counts()

df['application_type'].replace(['INDIVIDUAL', 'JOINT', 'DIRECT_PAY'], [1,0,1], inplace = True)

df

df.info()

"""## Treating missing values"""

df.isna().sum()

df['emp_title'].fillna(df['emp_title'].mean(),inplace = True)

sns.boxplot(y = df['revol_util'])

df['revol_util'].fillna(df['revol_util'].median(),inplace = True)

"""## Data preparation and preprocessing"""

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df.drop(columns = ['address']))

scaled_data = pd.DataFrame(scaled_data, columns = df.drop(columns = ['address']).columns)

scaled_data

"""## Filling missing values using KNN Imputer

### The reason we have done normalisation before imputing is because KNN is a distance based algorithm. So we want all features to be equally important. We don't want a single feature to dominate.
"""

from sklearn.impute import KNNImputer

imputer = KNNImputer(n_neighbors = 12)
df_imputed = imputer.fit_transform(scaled_data)
df_imputed = pd.DataFrame(df_imputed, columns = scaled_data.columns)
df_imputed

"""## Univariate Analysis"""

sns.kdeplot(x = df_imputed['loan_amnt'])

"""### Treating outliers by clipping"""

df['loan_amnt'][df['loan_amnt'] >=3] = 3
sns.kdeplot(x = df_imputed['loan_amnt'])

df['loan_amnt'].max()

for columns in df_imputed.columns:
    sns.kdeplot(x = df_imputed[columns])
    plt.show()

"""## Visualising outliers using Box plots"""

for columns in df_imputed.columns:
    sns.boxplot(y = df_imputed[columns])
    plt.show()

df_imputed.columns

"""## Feature Engineering - Transforming the data

### For variables which have skewed distribution, we will use box-cox transformation
"""

from scipy.stats import boxcox
scaler = StandardScaler()
def trans(df):
    df = df - np.min(df) +1
    for col in df.columns:
        df[col], lambda_ = boxcox(df[col])
        df_ = pd.DataFrame(scaler.fit_transform(df), columns = df.columns)

    return df_

"""### For variables which are normally distributed, we will perform clipping to remove outliers"""

def clip(df):
    for col in df.columns:
        df.loc[df[col]>=3,col] = 3
        df.loc[df[col]<=-3,col] = -3
    return df

df_boc_cox = trans(df_imputed[['annual_inc','dti', 'open_acc','revol_bal', 'revol_util', 'total_acc']])
df_clip = clip(df_imputed[['loan_amnt', 'int_rate', 'installment', 'sub_grade','emp_title', 'date', 'earliest_cr_line_date']])
df_lef_out = df[['term', 'verification_status', 'loan_status', 'pub_rec',
                'initial_list_status','pub_rec_bankruptcies',
                'mortage_one_hot', 'rent_one_hot', 'own_home', 'application_type']]

df_new = pd.concat([df_boc_cox, df_clip, df_lef_out], axis = 1)
for columns in df_new.columns:
    sns.kdeplot(x = df_new[columns])
    plt.show()

df_new[['annual_inc','dti', 'open_acc','revol_bal', 'revol_util', 'total_acc']] = clip(df_new[['annual_inc','dti', 'open_acc','revol_bal', 'revol_util', 'total_acc']])
for columns in df_new.columns:
    sns.kdeplot(x = df_new[columns])
    plt.show()

"""## Treating Outliers in multi-dimensional plane using Gaussian Mixture Models"""

from sklearn.mixture import GaussianMixture

"""### Before we perform GMM, we need to understand how many classes exist in the dataset. To find the optimum number we find out the AIC and BIC score and find the elbow point."""

bics = []
aics = []
for n in range(1,10):
    gmm = GaussianMixture(n_components = n)
    gmm.fit(df_new.drop(columns = 'loan_status'))
    bics.append(gmm.bic(df_new.drop(columns = 'loan_status')))
    aics.append(gmm.aic(df_new.drop(columns = 'loan_status')))

plt.plot( np.arange(1,10), bics, label='BIC')
plt.plot( np.arange(1,10), aics, label='AIC')
plt.xlabel('Number of Components')
plt.ylabel('Scores')
plt.legend()
plt.title('Elbow Method Using BIC/AIC')
plt.show()

"""### As we can see we don't find any significant decline in AIC/BIC. But our intuition tells us that it should be 2 classes - the ones who fully paid the loan and ones who didn't."""

gmm = GaussianMixture(n_components = 2)
gmm.fit(df_new.drop(columns = 'loan_status'))
prob = gmm.score_samples(df_new.drop(columns = 'loan_status'))

threshold = np.percentile(prob, 1)

"""### Let us remove the data points whose probability of lying inside the cluster is less than 1%"""

df_new = df_new[prob >= threshold]

"""## Bivariate Analysis"""

sns.pairplot(df_new, size = 4)

sns.heatmap(df_new.corr(), cmap ="crest")

# The above pairplot contains too many columns, so let us only focus on what is relavant.

sns.pairplot(df_new[['annual_inc','dti', 'open_acc','revol_bal', 'revol_util', 'total_acc','loan_amnt', 'int_rate', 'installment', 'sub_grade','emp_title', 'date', 'earliest_cr_line_date']], size = 4)

# This still looks unreadable, so let us play around with point sizes.

sns.scatterplot(x = df_new['loan_amnt'], y = df_new['annual_inc'], size = 0.2)

sns.scatterplot(x = df_new['loan_amnt'], y = df_new['annual_inc'], s = 0.2)

df2 = df_new[['annual_inc','dti', 'open_acc','revol_bal', 'revol_util', 'total_acc','loan_amnt', 'int_rate', 'installment', 'sub_grade','emp_title', 'date', 'earliest_cr_line_date']]
n = len(df2.columns)
for i in range(n):
    if i == (n-1):
        break
    for j in range(i+1,n):
        sns.scatterplot(x = df2.iloc[:,i], y = df2.iloc[:,j], s = 0.4)
        plt.show()

n = len(df2.columns)
df2.iloc[:,3]

df_new.drop(columns = ['installment', 'sub_grade', 'pub_rec_bankruptcies'],inplace = True)

"""## Let us drop duplicate values"""

df_new.drop_duplicates(inplace = True)

df_new

"""## Checking for Multicollinearity"""

import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
# Add a constant to the predictors
df_new = sm.add_constant(df_new)

# Calculate VIF scores
vif_data = pd.DataFrame()
vif_data['Feature'] = df_new.columns
vif_data['VIF'] = [variance_inflation_factor(df_new.values, i) for i in range(df_new.shape[1])]

# Display the VIF scores
print(vif_data)

# Looks like rent_one_hot column is easily predictable by other vatiables, so lets drop it and again run the vif
df_new2 = df_new.drop(columns = [ 'rent_one_hot'])

# Calculate VIF scores
vif_data = pd.DataFrame()
vif_data['Feature'] = df_new2.columns
vif_data['VIF'] = [variance_inflation_factor(df_new2.values, i) for i in range(df_new2.shape[1])]

# Display the VIF scores
print(vif_data)

df_new2 = df_new.drop(columns = [ 'rent_one_hot', 'application_type'])

# Calculate VIF scores
vif_data = pd.DataFrame()
vif_data['Feature'] = df_new2.columns
vif_data['VIF'] = [variance_inflation_factor(df_new2.values, i) for i in range(df_new2.shape[1])]

# Display the VIF scores
print(vif_data)

df_new2 = df_new.drop(columns = [ 'rent_one_hot', 'application_type','term'])

# Calculate VIF scores
vif_data = pd.DataFrame()
vif_data['Feature'] = df_new2.columns
vif_data['VIF'] = [variance_inflation_factor(df_new2.values, i) for i in range(df_new2.shape[1])]

# Display the VIF scores
print(vif_data)

# We see that these 3 columns have a high vif, so lets drop them and
df_new.drop(columns = [ 'rent_one_hot', 'application_type','term'], inplace = True)

df_new

"""## Building the base model

### Let us evaluate the model on recall, precision, accuracy, f1 score and specifity using K Folf cross validation.
"""

from sklearn.model_selection import KFold, cross_validate
from sklearn.metrics import f1_score, recall_score, precision_score, accuracy_score, make_scorer

def specificity(y_true, y_pred):
    tn = np.sum(y_pred[y_true == 0]==0)
    fp = np.sum(y_pred[y_true == 0]==1)
    return tn/(tn+fp)

model = LogisticRegression()
kfold = KFold(n_splits = 5)
x_train, x_test, y_train, y_test = train_test_split(df_new.drop(columns = 'loan_status'), df_new['loan_status'], test_size = 0.2)
specificity = make_scorer(specificity)
scoring = {
    'recall': 'recall',
    'precision': 'precision',
    'accuracy': 'accuracy',
    'f1_score': 'f1',
    'specificity': specificity
}

results = cross_validate(estimator = model, X = x_train, y = y_train, scoring = scoring, cv = kfold)

results

print('recall', np.mean(results['test_recall']))
print('precision', np.mean(results['test_precision']))
print('accuracy', np.mean(results['test_accuracy']))
print('f1 score', np.mean(results['test_f1_score']))
print('specificity', np.mean(results['test_specificity']))

"""## We see that model is performing really well on recall, precision, f1 score. But very poorly on specificity. This shows that the model is not able to classifiy negative class (here it is 0)."""

# Let us save the final dataset
df_new.to_csv('df_new_loan_tab', index = False)

df_new = pd.read_csv('df_new_loan_tap')

df_new

model = LogisticRegression()
model.fit(x_train, y_train)

model.coef_

df_new.columns

"""## Printing the coefficients of the variables"""

pd.DataFrame({'columns': df_new.drop(columns = 'loan_status').columns, 'coefficients':model.coef_[0]})

"""## Let us find the optimum threshold for classification."""

y_pred = model.predict_proba(x_train)[:,1]

threshold = np.linspace(0,1,101)

tpr = []
fpr = []
for t in threshold:
    y_tr = np.array(y_train)
    y_pr = np.where(y_pred>=t,1,0)
    tp = 0
    fp = 0
    tn = 0
    fn = 0
    for i in range(len(y_pr)):
        if ((y_pr[i] == 1) and (y_tr[i] == 1)):
            tp = tp +1
        elif ((y_pr[i] == 0) and (y_tr[i] == 1)):
            fn = fn +1
        elif ((y_pr[i] == 0) and (y_tr[i] == 0)):
            tn = tn +1
        else:
            fp = fp +1
    fpr.append(fp/(fp+tn))
    tpr.append(tp/(tp+fn))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve' )
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""### We see that the model is performing better than a randomly guessing model. The optimum threshold would be something near the top left corner which maximises TPR but minimises FPR."""

plt.plot(fpr, tpr)

"""## Let us make precision recall curve."""

recall = []
precision = []
for t in threshold[0:-1]:
    y_tr = np.array(y_train)
    y_pr = np.where(y_pred>=t,1,0)
    tp = 0
    fp = 0
    tn = 0
    fn = 0
    for i in range(len(y_pr)):
        if ((y_pr[i] == 1) and (y_tr[i] == 1)):
            tp = tp +1
        elif ((y_pr[i] == 0) and (y_tr[i] == 1)):
            fn = fn +1
        elif ((y_pr[i] == 0) and (y_tr[i] == 0)):
            tn = tn +1
        else:
            fp = fp +1
    precision.append(tp/(tp+fp))
    recall.append(tp/(tp+fn))

plt.figure()
plt.plot(recall, precision, color='darkorange', lw=2, label='precision-recall' )
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('recall')
plt.ylabel('precision')
plt.title('precision-recall Curve')
plt.legend(loc="lower right")
plt.show()

"""## Let us make precision recall curve using sklearn"""

from sklearn.metrics import precision_recall_curve, average_precision_score
precision, recall, _ = precision_recall_curve(y_train, y_pred)
average_precision = average_precision_score(y_train, y_pred)

plt.figure()
plt.plot(recall, precision, color='b', lw=2, label='Precision-Recall curve (area = %0.2f)' % average_precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

"""## Let us find the optimum threshold for maximising the f1 score."""

precision, recall, thresholds = precision_recall_curve(y_train, y_pred)
f1_scores = 2*recall*precision / (recall + precision)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print('Best Threshold by F1 Score:', optimal_threshold)

# Plotting Precision-Recall Curve
plt.plot(thresholds, precision[:-1], 'b--', label='Precision')
plt.plot(thresholds, recall[:-1], 'g-', label='Recall')
plt.xlabel('Threshold')
plt.legend(loc='upper left')
plt.title('Precision-Recall vs Threshold')
plt.show()

y_pred = model.predict_proba(x_test)[:,1]
y_tr = np.array(y_test)
y_pr = np.where(y_pred>=optimal_threshold,1,0)
tp = 0
fp = 0
tn = 0
fn = 0
for i in range(len(y_pr)):
    if ((y_pr[i] == 1) and (y_tr[i] == 1)):
        tp = tp +1
    elif ((y_pr[i] == 0) and (y_tr[i] == 1)):
        fn = fn +1
    elif ((y_pr[i] == 0) and (y_tr[i] == 0)):
        tn = tn +1
    else:
        fp = fp +1
recall = tp/(tp+fn)
precision = tp/(tp+fp)
specificity = tn/(tn + fp)
accuracy = (tp+tn)/(tp+fp+tn+fn)
f1_score = 2*recall*precision/(precision + recall)

recall, precision, specificity, accuracy, f1_score

"""## We see that tuning the threshold gives us a better model with respect to specificity and f1 score.

## Confusion matrix for the base model
"""

confusion_matrix = pd.DataFrame({"Actual positives": [tp,fn], "Actual negatives": [fp,tn]})
confusion_matrix.index = ["Predicted positives", "Predicted negatives"]

confusion_matrix

100*confusion_matrix/(tp+tn+fp+fn)

"""### We observe one very important point here - the model is not able to classify negative class. This is shown by a large false positive value from the confusion matrix. This is also proved by low value of specificity. The key reason for this is that negative class is the minority class making up just 20% of the total data.
### To overcome this we will have to do 2 things:
### 1. Oversample the minority class using SMOTE
### 2. Balancing the weight of minority class in the cost function.
"""

pip install imbalanced-learn

from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE

"""### We will only sample the minority class to 50% of the majority class. The reason being, if we had done too much oversampling, the model will overfit."""

# We are using SMOTE + ENN model
smote = SMOTE(sampling_strategy = 0.5)
smote_enn = SMOTEENN(smote = smote)

x_train_s, y_train_s = smote_enn.fit_resample(x_train, y_train)

x_train

# We have the oversampled data
x_train_s

y_train.value_counts()

y_train_s.value_counts()

def specificity(y_true, y_pred):
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    tn = np.sum(y_pred[y_true == 0]==0)
    fp = np.sum(y_pred[y_true == 0]==1)
    return tn/(tn+fp)

"""### Let's again train a new model which is able to balance the minority class in cost function and hopefully give us better results"""

model_weight_balance = LogisticRegression(class_weight = "balanced")
kfold = KFold(n_splits = 5)
scoring = {
    'recall': 'recall',
    'precision': 'precision',
    'accuracy': 'accuracy',
    'f1_score': 'f1',
    'specificity': specificity
}

results = cross_validate(estimator = model_weight_balance, X = x_train_s, y = y_train_s, scoring = scoring, cv = kfold)
results

"""### This is giving us very good result, but the issue here is we are evaluating it on train data where there is a high chance of overfitting. Let us test on test data."""

model_weight_balance = LogisticRegression(class_weight = "balanced")
model_weight_balance.fit(x_train_s, y_train_s)

y_pred_train = model_weight_balance.predict(x_train_s)
y_pred_test = model_weight_balance.predict(x_test)

print("recall train = %f, recall test = %f" %(recall_score(y_train_s, y_pred_train), recall_score(y_test, y_pred_test)))
print("accuracy train = %f, accuracy test = %f" %(accuracy_score(y_train_s, y_pred_train), accuracy_score(y_test, y_pred_test)))
print("precision train = %f, precision test = %f" %(precision_score(y_train_s, y_pred_train), precision_score(y_test, y_pred_test)))
print("f1 score train = %f, f1 score test = %f" %(f1_score(y_train_s, y_pred_train), f1_score(y_test, y_pred_test)))
print("specificity train = %f, specificity test = %f" %(specificity(y_train_s, y_pred_train), specificity(y_test, y_pred_test)))

"""## The model is now much better at specificity. But there is a big difference in train and test data metrics. This hints at overfitting.

### Let us create another model without smote and just using the weight balance. This will hopefully give us better results without overfitting.
"""

model2 = LogisticRegression(class_weight = "balanced")
model2.fit(x_train, y_train)

y_pred_train = model2.predict(x_train)
y_pred_test = model2.predict(x_test)

print("recall train = %f, recall test = %f" %(recall_score(y_train, y_pred_train), recall_score(y_test, y_pred_test)))
print("accuracy train = %f, accuracy test = %f" %(accuracy_score(y_train, y_pred_train), accuracy_score(y_test, y_pred_test)))
print("precision train = %f, precision test = %f" %(precision_score(y_train, y_pred_train), precision_score(y_test, y_pred_test)))
print("f1 score train = %f, f1 score test = %f" %(f1_score(y_train, y_pred_train), f1_score(y_test, y_pred_test)))
print("specificity train = %f, specificity test = %f" %(specificity(y_train, y_pred_train), specificity(y_test, y_pred_test)))

"""### Model 2 gives us very good result on both positive and negative class without overfitting."""

# This is the base model metrics
model = LogisticRegression()
model.fit(x_train, y_train)
y_pred_train = model.predict(x_train)
y_pred_test = model.predict(x_test)

print("recall train = %f, recall test = %f" %(recall_score(y_train, y_pred_train), recall_score(y_test, y_pred_test)))
print("accuracy train = %f, accuracy test = %f" %(accuracy_score(y_train, y_pred_train), accuracy_score(y_test, y_pred_test)))
print("precision train = %f, precision test = %f" %(precision_score(y_train, y_pred_train), precision_score(y_test, y_pred_test)))
print("f1 score train = %f, f1 score test = %f" %(f1_score(y_train, y_pred_train), f1_score(y_test, y_pred_test)))
print("specificity train = %f, specificity test = %f" %(specificity(y_train, y_pred_train), specificity(y_test, y_pred_test)))

"""### Let us train another model without ENN and with just regular SMOTE."""

smote = SMOTE(sampling_strategy = 0.5)
x_train_s, y_train_s = smote.fit_resample(x_train, y_train)
model_weight_balance = LogisticRegression(class_weight = "balanced")
kfold = KFold(n_splits = 5)
scoring = {
    'recall': 'recall',
    'precision': 'precision',
    'accuracy': 'accuracy',
    'f1_score': 'f1',
    'specificity': specificity
}

results = cross_validate(estimator = model_weight_balance, X = x_train_s, y = y_train_s, scoring = scoring, cv = kfold)
results

model_weight_balance.fit(x_train_s, y_train_s)

y_pred_train = model_weight_balance.predict(x_train_s)
y_pred_test = model_weight_balance.predict(x_test)

print("recall train = %f, recall test = %f" %(recall_score(y_train_s, y_pred_train), recall_score(y_test, y_pred_test)))
print("accuracy train = %f, accuracy test = %f" %(accuracy_score(y_train_s, y_pred_train), accuracy_score(y_test, y_pred_test)))
print("precision train = %f, precision test = %f" %(precision_score(y_train_s, y_pred_train), precision_score(y_test, y_pred_test)))
print("f1 score train = %f, f1 score test = %f" %(f1_score(y_train_s, y_pred_train), f1_score(y_test, y_pred_test)))
print("specificity train = %f, specificity test = %f" %(specificity(y_train_s, y_pred_train), specificity(y_test, y_pred_test)))

"""### This model is surprisingly performing really well, but we are compromising on specificity.

## So the best model that gave us the best result is Model2 which is having weight balancing.
"""

from sklearn.metrics import confusion_matrix
model2 = LogisticRegression(class_weight = "balanced")
model2.fit(x_train, y_train)

y_pred_train = model2.predict(x_train)
y_pred_test = model2.predict(x_test)

"""## Let us tune the threshold."""

y_pred = model2.predict_proba(x_train)[:,1]

tpr = []
fpr = []
for t in threshold:
    y_tr = np.array(y_train)
    y_pr = np.where(y_pred>=t,1,0)
    tp = 0
    fp = 0
    tn = 0
    fn = 0
    for i in range(len(y_pr)):
        if ((y_pr[i] == 1) and (y_tr[i] == 1)):
            tp = tp +1
        elif ((y_pr[i] == 0) and (y_tr[i] == 1)):
            fn = fn +1
        elif ((y_pr[i] == 0) and (y_tr[i] == 0)):
            tn = tn +1
        else:
            fp = fp +1
    fpr.append(fp/(fp+tn))
    tpr.append(tp/(tp+fn))

plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve' )
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_recall_curve, average_precision_score
precision, recall, _ = precision_recall_curve(y_train, y_pred)
average_precision = average_precision_score(y_train, y_pred)

plt.figure()
plt.plot(recall, precision, color='b', lw=2, label='Precision-Recall curve (area = %0.2f)' % average_precision)
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve')
plt.legend(loc="lower left")
plt.show()

precision, recall, thresholds = precision_recall_curve(y_train, y_pred)
f1_scores = 2*recall*precision / (recall + precision)
optimal_idx = np.argmax(f1_scores)
optimal_threshold = thresholds[optimal_idx]

print('Best Threshold by F1 Score:', optimal_threshold)

# Plotting Precision-Recall Curve
plt.plot(thresholds, precision[:-1], 'b--', label='Precision')
plt.plot(thresholds, recall[:-1], 'g-', label='Recall')
plt.xlabel('Threshold')
plt.legend(loc='upper left')
plt.title('Precision-Recall vs Threshold')
plt.show()

"""### The optimum threshold is 0.24 for this model."""

y_pr = model2.predict(x_test)
y_tr = np.array(y_test)
tp = 0
fp = 0
tn = 0
fn = 0
for i in range(len(y_pr)):
    if ((y_pr[i] == 1) and (y_tr[i] == 1)):
        tp = tp +1
    elif ((y_pr[i] == 0) and (y_tr[i] == 1)):
        fn = fn +1
    elif ((y_pr[i] == 0) and (y_tr[i] == 0)):
        tn = tn +1
    else:
        fp = fp +1
recall = tp/(tp+fn)
precision = tp/(tp+fp)
specificity = tn/(tn + fp)
accuracy = (tp+tn)/(tp+fp+tn+fn)
f1_score = 2*recall*precision/(precision + recall)

recall, precision, specificity, accuracy, f1_score

confusion_matrix = pd.DataFrame({"Actual positives": [tp,fn], "Actual negatives": [fp,tn]})
confusion_matrix.index = ["Predicted positives", "Predicted negatives"]

100*confusion_matrix/(tp + tn + fp + fn)

"""### From the confusion matrix we can see that we have significantly reduced the false positives but at the same time we are able to maintain the true positive and true negatives."""

df

"""# Questionnaire"""

100*df["loan_status"].value_counts()/df["loan_status"].value_counts().sum()

df[["loan_amnt", "installment"]].corr()

df["home_ownership"].value_counts()

df.groupby("home_ownership")["loan_status"].agg("mean")

df.replace({"Fully Paid":1, "Charged Off": 0}, inplace = True)

df.groupby("grade")["loan_status"].agg("mean")

df.loc[df["loan_status"] ==1,:].groupby("emp_title")["loan_status"].agg("count").sort_values(ascending = False)

df['state'] = df['address'].str.extract(r', (\w{2}) ')

df['state']

df.groupby("state")["loan_status"].agg("mean").sort_values(ascending = False)

"""# Insights and Recommendations

## 1. The percentage of people paying back the amount is 80% and those who didn't pay back is 20%. Which is a pretty big number. The bank should reduce the money given to unpaid people.  
## 2. People whose home ownership is NONE or RENT have low chances of paying back. Whereas those having Mortage or OTHER have relatively better chances of paying back. So the Bank should focus on such customers.
## 3. We see that there is a high correlation between the grade and payback. The Bank can prioritize on this parameter to decide whether to give or not.
## 4. Also once we determine the probabilty of a person paying back. Bank can then set the interest rate before disbursing the loan. Because higher interest rate is highly correlated with payback percentage. Low interest rate -> good candidate, high interest rate -> risky candidate.
## 5. The Parameters that dont have a significant impact to determine credit worthiness are - Grade (since subgrade already provides the info), address, Purpose/title (it is text data and not very useful for logistic regression model), rent_one_hot (because it can be determined by those who dont have mortage or own home), application_type,term (because it is got from loan amount and duration).
## 6. After training the logistic regression model, the coefficient we get determines how important a parameter is. The Bank has to focus more on the parameters - Employee title, verification status, dti and mortage_one_hot. The parameters that are not so important are - initial_list_status, earliest_cr_line_date, public record, total accounts and revol_balance.
## 7. If bank focuses on precision they would not give to people who won't return back the money, this will reduce NPA. Low false positive rate will give us higher precision and more profit to bank. To achieve this we can use SMOTE with weight balancing model. This approach is effective when the bank wants to play safe.
## 8. If bank wants to maintain customer relation/satisfaction they could give loan to more risky customer, this way they will be less likely to reject a good customer. To achieve this, bank needs to focus on recall and reduce false negative rate. Model best suited for this is the one without oversampling and weight balancing.
## 9. If the Bank wants a best overall model with high precision and recall then I would suggest to use model2 with weight balancing and without oversampling. Using this we could be 80% sure that we are correctly classifying positive as positive and negative as negative. This model has good recall as well as specificity.
## 10. Threshold can be adjusted to increase precision and decrease recall or vice versa. The ideal threshold to maximise both precision and recall and thus maximise f1 score is 0.57 for model 1 (without oversampling/weight balancing) and 0.27 for model 2 (with oversampling/weight balancing).
"""

